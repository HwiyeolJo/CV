# Hwiyeol Jo

Currently at
- NAVER Cloud as Research Scientist
- Konkuk University as Adjunct Professor<!-- (Search US) -->

\* Reach me by email: hwiyeolj@gmail .com
 
## Research Interests
Artificial Intelligence with Cognitive Science

Machine Learning for Natural Language Processing to investigate human minds

### Research Keyword
Knowledge Transfer through Representation Learning
- incorporating (lexical) semantics, information retrieval
- general machine learning frameworks for self-supervised representation learning
- (Large Language Models are no more research topic but research tools)

## Education
- M.S. in Computer Science & Engineering, Seoul National University, Republic of Korea
  - Master Thesis: Data Augmentation Technique with Knowledge Extraction for Text Question Answering by Deep Neural Networks [Advisor: Byoung-Tak Zhang]
- B.S. in Computer Science & Engineering, Konkuk University, Republic of Korea
+ Military service at Third Republic Of Korea Army; TROKA

## Research Experience

[Google Scholar](https://scholar.google.co.kr/citations?user=hFSHr0gAAAAJ&hl=ko)

### Preprint/Under Reviews

**Jo, H.**, Lee, J., Lee, J., Lee, S. W., Park, J., & Yoo, K. M. (2025). Finding Answers in Thought Matters: Revisiting Evaluation on Large Language Models with Reasoning. arXiv preprint arXiv:2510.14773.

NAVER Cloud HyperCLOVA X Team (2025). HyperCLOVA X THINK Technical Report. arXiv preprint arXiv:2506.22403.
- Core Contributor

Park, C., Kim, H., Kim, J., Kim, Y., Kim, T., Cho, H., **Jo, H.**, Lee, S., Yoo, K. M. (2024). Investigating the Influence of Prompt-Specific Shortcuts in AI Generated Text Detection. ArXived.

- Showed unrobustness of previous AI-Generated Text detectors with prompt variations.
- Proposed Feedback-based Adversarial Instruction List Optimization (FAILOpt) for general attack method.
- Additional training with FAILOpt prompt improves models' robustness.

**Jo, H.**, & Zhang, B. (2019). Ruminating Word Representations with Random Masker. ArXived.

- Proposed an iterative training method for better word representation and for model regularization

### Accepted

#### 1st Authored: ACL(2), EMNLP(4), NAACL(1), CogSci(1), WS(2)

**Jo, H.**, Lee, H., Yoo, K. M., & Park, T. (2025). ZeroDL: Zero-shot Distribution Learning for Text Clustering via Large Language Models. ACL2025(Findings)

- We propose a novel framework called Zero-shot Distribution Learning framework to understands data distribution across data.
  - Zero-shot inferences to generate meta-level information.
  - Aggragation for learning the data distribution.

**Jo, H.** et al., (2025). Taxonomy and Analysis of Sensitive User Queries in Generative AI Search. NAACL2025(Findings)

- Defined a taxonomy with sensitive query categories for national-scale search engine, develop a query classifier
- Analyze the distribution of input queries to provide insights.

**Jo, H.** (2023). Self-supervised Post-processing Method to Enrich Pretrained Word Vectors. EMNLP2023(Findings) [Selected for Presentation]

- Extended extrofitting without requiring any external lexicon; Self-supervised extrofitting
- Improved on various benchmarks like word similarity tasks in various language, DST, and text classification, showing the quality of the approach

**Jo, H.** (2023). A Self-Supervised Integration Method of Pretrained Language Models and Word Definitions. ACL2023(Findings)

- Dissected BERT representation with respect to the basic form of semantics, formal definition
- Developed DefBERT; The method to inject formal definition into contextualized models
- Proposed DefRank, SenseRank tasks to measure the ability to incorporate formal definitions

**Jo, H.**, Kang, D., Heads, A., & Hearst, M. (2021). Modeling Mathematical Notation Semantics in Academic Papers. EMNLP2021(Findings)

- Proposed two notation prediction tasks to test models’ mathematical semantics understanding
- Presented a fine-tuned model mathBERT for the tasks, which shows much better performance than pretrained BERT
- Showed in-depth analyses of the results

**Jo, H.**\*, Lim, J.\*, & Zhang, B. (2021). Devil’s Advocate: Novel Boosting Ensemble Method from Psychological Findings for Text Classification. EMNLP2021(Findings) (*Equal Contribution)

- Applied a novel ensemble method based on group discussion theory, Devil’s Advocate

Lim, J.\*, **Jo, H.**\*, Zhang, B., & Park, J. (2021). Passive Versus Active: Frameworks of Active Learning for Linking Humans to Machines.  The 43rd Annual Meeting of the Cognitive Science Society. (*Equal Contribution)

- Analyzed active learning on both Humans and Machines
- Proposed two experiments (human and machine, respectively) in order to compare active and passive learning.

**Jo, H.**, & Cinarel, C. (2019). Delta-training: Simple Semi-Supervised Text Classification using Pretrained Word Embeddings. EMNLP2019

- Suggested a semi-supervised training framework using knowledge gaps between two classifiers

**Jo, H.**, & Ryu, J. (2018). Psychological State in Text: A Limitation of Sentiment Analysis. IJCAI-ECAI Workshop on AI and Computational Psychology: Theories, Algorithms and Applications (CompPsy). [Extended Abstract]

- Presented the limitation of current sentiment analysis method; The model is not able to model psychological state of the person.

**Jo, H.**, & Choi, S. J. (2018). Extrofitting: Enriching Word Representation and its Vector Space with Semantic Lexicons. In Proceedings of the 3rd ACL Workshop on Representation Learning for NLP (RepL4NLP).

- Developed extrofitting that injects lexical semantics into word vectors
- Extrofitting results in word vector generalization

#### Co-authored: EMNLP(1), EMNLP-Industry(1), WS(1)

Kwon, O., Jeon D., Choi, N., Cho, G., **Jo, H.**, Kim, C., Lee, H., Kang, I., Kim, S., & Park, T. (2024). SLM as Guardian: Pioneering AI Safety with Small Language Model. EMNLP2024(Industry Track)

Kim, J., Kim, H. J., Cho, H., **Jo, H.**, Lee, S. W., Lee, S. G., ... & Kim, T. (2022). Ground-Truth Labels Matter: A Deeper Look into Input-Label Demonstrations. EMNLP2022

- Investigated input-label importance of large-scale pretrained language model (e.g., GPTs) in few-shot settings

Lim, J.\*, **Jo, H.**\*, Zhang, B. & Park, J. (2020). Human-Like Active Learning: Machines Simulating Human Learning Process. NeurIPS 2020 Workshop on BabyMind (*Equal Contribution) [Spotlight Presentation]

- Extended to CogSci2021

#### Publication Lists with Psychology related papers are omitted.

## Work Experiences

[2025.05 – Current] Research Scientist, NAVER Cloud, Republic of Korea
- Investigating novel and efficient way of using foundation models

[2024.09 - Current] Adjunct Professor, Graduate School of Information & Communications, Konkuk University, Republic of Korea

[2024.02 – 2025.04] Research Scientist & Engineer, NAVER, Republic of Korea
- Improving Search Engine results with self/unsupervised methods using LLMs' internal knowledge.

[2024.03 - 2024.08] Lecturer, Department of Data Science, Hanyang University, Republic of Korea

[2022.11 – 2024.02] Research Scientist & Engineer, Search US, NAVER, Republic of Korea
- Large Language Model backbone (with deepspeed+Megatron) and its applications (with finetuning+prompting)
- Improving Search Engine results, using word/model semantics
- Input-level sensitivity checking for AI safety

[2021.07 – 2022.11] Research Scientist & Engineer, Clova CIC, NAVER, Republic of Korea

- General MRC Models to extract information related to query from given documents
- Made it component model of information retrieval filtering and question generation model

[2019.07 – 2021.07] Research Assistant, Institute of Computer Technology, Seoul National University, Republic of Korea

- Leader of Video Intelligent Platform Team; Build full stack of AI Demo system
- Demo video: https://youtu.be/ElzgeQIZF0s

[2020.03 – 2021.02] Lecturer, Department of Computer Education, Seoul National University of Education, Republic of Korea

[2020.03 – 2020.08] Adjunct Professor, Computer Engineering Department, Hongik University, Republic of Korea

[2019.02 – 2019.07] Researcher, AI Division, LG Sciencepark, Republic of Korea

- Research regarding data augmentation
- PoC on recent models/algorithms

[2017.07 – 2019.02] Researcher, Artificial Intelligence Lab, LG Electronics CTO, Republic of Korea

- Word vector generation and post-processing for target tasks (e.g., Chatbot embedded in TV products, and disorder symptom classifier for customer service center)
- PoC on recent models/algorithms and related company (e.g., Luminoso)
- Research regarding word vector specialization and document representation

## Teaching Courses

Advanced AI (Natural Language Processing), Graduate School of Information & Communications, Konkuk University, Spring 2025.

Introduction to Deep Neural Networks, Graduate School of Information & Communications, Konkuk University, Fall 2024.

Deep Learning and its Application (NLP), Department of Data Science, Hanyang University, Spring 2024.

App Programming, Department of Computer Education, Seoul National University of Education. Fall 2020.

Introduction to Computer Engineering, Department of Computer Engineering, Hongik University. Spring 2020.

Computing and Mathematics, Department of Computer Education, Seoul National University of Education. Spring 2020.

## Academic Services

ACL: 2023-Current

EMNLP: 2021-Current

NAACL: 2024-Current

CogSci: 2022, 2023, 2024

COLING: 2022, 2024 (with LREC),

CoLM: 2024-Current

## Talks

Large Language Model Trends and Applications in NLP, Kyoung-Hee University, July 4th, 2024.

Post-processing Approaches Toward Better Embeddings, Bird-of-Feather session on Embeddings in EMNLP2023, Dec 9th, 2023.

AI using Python, Seoul National University of Education, Jan 16-17th, 2020.

Computational Linguistic Approaches for Sentiment Analysis, Cognitive Science Colloquium in Seoul National University, June 3rd, 2019.

Large-Scale Text Classification with Deep Neural Networks, Korea Institute of S&T Evaluation and Planning, Feb 1st, 2017.

Introduction to IoT, Drone, and Cognitive Science, National Association of Cognitive Science Industries, Seojeong University, Feb 23–25th, 2016.

## Computer Skills

Python [Fluent] (I have teaching experiences)

Pytorch [Fluent] (I have teaching experiences)

Tensorflow [Advanced] (I am able to understand how the codes are working, and modify them)

C, C++, Java [Intermediate] (It’s been for a long time I use them)
